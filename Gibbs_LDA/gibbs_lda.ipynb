{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 3: Gibbs Sampling and LDA\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Write a function that accepts data $y$, prior parameters $\\nu$, $\\tau^2$, $\\alpha$, and $\\beta$, and an integer $n$. Use Gibbs sampling to generate $n$ samples of $\\mu$ and $\\sigma^2$ for the data in `examscores.npy`.\n",
    "\n",
    "Test your sampler with priors $\\nu=80$, $\\tau^{2} = 16$, $\\alpha = 3$, and $\\beta = 50$, collecting $1000$ samples.\n",
    "Plot your samples of $\\mu$ and your samples of $\\sigma^{2}$ versus the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(y, nu, tau2, alpha, beta, n_samples):\n",
    "    \"\"\" Gibbs sampler for the exam scores problem, assuming the\n",
    "    following likelihood and priors.\n",
    "        y_i    ~ N(mu, sigma2),\n",
    "        mu     ~ N(nu, tau2),\n",
    "        sigma2 ~ IG(alpha, beta),\n",
    "\n",
    "    Parameters:\n",
    "        y ((N,) ndarray): the exam scores.\n",
    "        nu (float): The prior mean parameter for mu.\n",
    "        tau2 (float > 0): The prior variance parameter for mu.\n",
    "        alpha (float > 0): The prior alpha parameter for sigma2.\n",
    "        beta (float < 0): The prior beta parameter for sigma2.\n",
    "        n_samples (int): the number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "        ((n_samples, 2) ndarray): The mu and sigma2 samples (as columns).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Problem 1 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Plot the kernel density estimators for the posterior distributions of $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "Next, use your samples of $\\mu$ and $\\sigma^2$ to draw samples from the posterior predictive distribution.\n",
    "Plot the kernel density estimator of your sampled scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Complete the method `LDACGS._initialize()`.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Complete the method `LDACGS._sweep()`.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "Complete the method `LDACGS.sample()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDACGS:\n",
    "    \"\"\" Do LDA with Gibbs Sampling. \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, alpha=0.1, beta=0.1):\n",
    "        \"\"\" Initializes attributes n_topics, alpha, and beta. \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def _buildCorpus(self, filename, stopwords_file=None):\n",
    "        \"\"\" Reads the given filename, and using any provided stopwords,\n",
    "            initializes attributes vocab and documents. In this lab,\n",
    "            each line of filename is considered a document.\n",
    "            \n",
    "            vocab is a list of terms found in filename.\n",
    "            \n",
    "            documents is a list of dictionaries (a dictionary for each \n",
    "            document); for dictionary m in documents, each entry is of \n",
    "            the form n:v, where v is the index in vocab of the nth word \n",
    "            in document m.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as infile:  # Create vocab\n",
    "            doclines = [line.rstrip().lower().split(' ') for line in infile]\n",
    "        n_docs = len(doclines)\n",
    "        self.vocab = list({v for doc in doclines for v in doc})\n",
    "\n",
    "        self.docs = doclines  # Save the documents for toplines()\n",
    "\n",
    "        if stopwords_file:   # If there are stopwords, remove them from vocab\n",
    "            with open(stopwords_file, 'r') as stopfile:\n",
    "                stops = stopfile.read().split()\n",
    "            self.vocab = [x for x in self.vocab if x not in stops]\n",
    "            self.vocab.sort()\n",
    "        \n",
    "        self.documents = []  # Create documents\n",
    "        for i in range(n_docs):\n",
    "            self.documents.append({})\n",
    "            for j in range(len(doclines[i])):\n",
    "                if doclines[i][j] in self.vocab:\n",
    "                    self.documents[i][j] = self.vocab.index(doclines[i][j])\n",
    "\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\" Initializes attributes n_words, n_docs, the three count matrices, \n",
    "            and the topic assignment dictionary topics.\n",
    "                        \n",
    "            Note that\n",
    "            n_topics = K, the number of possible topics\n",
    "            n_docs   = M, the number of documents being analyzed\n",
    "            n_words  = V, the number of words in the vocabulary\n",
    "            \n",
    "            To do this, you will need to initialize nkm, nkv, and nk \n",
    "            to be zero arrays of the correct size.\n",
    "            Matrix nkm corresponds to n_(k,m,.)\n",
    "            Matrix nkv corresponds to n_(k,.,v)\n",
    "            Matrix nk corresponds to n_(k,.,.)\n",
    "            You will then iterate through each word found in each document.\n",
    "            In the second of these for-loops (for each word), you will \n",
    "            randomly assign k as an integer from the correct range of topics.\n",
    "            Then, you will increment each of the count matrices by 1, \n",
    "            given the values for k, m, and v, where v is the index in \n",
    "            vocab of the nth word in document m.\n",
    "            Finally, assign topics as given.\n",
    "        \"\"\"\n",
    "        self.n_words = len(self.vocab)\n",
    "        self.n_docs = len(self.documents)\n",
    "                \n",
    "        # Initialize the three count matrices\n",
    "        # The (k, m) entry of self.nkm is the number of words in document m assigned to topic k\n",
    "        self.nkm = np.zeros((self.n_topics, self.n_docs))\n",
    "        # The (k, v) entry of self.nkv is the number of times term v is assigned to topic k\n",
    "        self.nkv = np.zeros((self.n_topics, self.n_words))\n",
    "        # The (k)-th entry of self.nk is the number of times topic k is assigned in the corpus\n",
    "        self.nk = np.zeros(self.n_topics)\n",
    "        \n",
    "        # Initialize the topic assignment dictionary\n",
    "        self.topics = {} # Key-value pairs of form (m,n):k\n",
    "        \n",
    "        random_distribution = np.ones(self.n_topics) / self.n_topics\n",
    "        for m in range(self.n_docs):\n",
    "            for n in self.documents[m]:\n",
    "                # Get random topic assignment, i.e. k = ...\n",
    "                # Increment count matrices\n",
    "                # Store topic assignment, i.e. self.topics[(m,n)]=k\n",
    "                raise NotImplementedError(\"Problem 3 Incomplete\")\n",
    "                \n",
    "                \n",
    "    def _sweep(self):\n",
    "        \"\"\" Iterates through each word of each document, giving a better\n",
    "            topic assignment for each word.\n",
    "            \n",
    "            To do this, iterate through each word of each document. \n",
    "            The first part of this method will undo what _initialize() did\n",
    "            by decrementing each of the count matrices by 1.\n",
    "            Then, call the method _conditional() to use the conditional \n",
    "            distribution (instead of the uniform distribution used \n",
    "            previously) to pick a more accurate topic assignment k.\n",
    "            Finally, repeat what _initialize() did by incrementing each of\n",
    "            the count matrices by 1, but this time using the more \n",
    "            accurate topic assignment.\n",
    "        \"\"\"\n",
    "        for m in range(self.n_docs):\n",
    "            for n in self.documents[m]:\n",
    "                # Retrieve vocab index for n-th word in document m\n",
    "                # Retrieve topic assignment for n-th word in document m\n",
    "                # Decrement count matrices\n",
    "                # Get conditional distribution\n",
    "                # Sample new topic assignment\n",
    "                # Increment count matrices\n",
    "                # Store new topic assignment\n",
    "                raise NotImplementedError(\"Problem 4 Incomplete\")\n",
    "\n",
    "                \n",
    "    def sample(self, filename, burnin=100, sample_rate=10, n_samples=10, stopwords_file=None):\n",
    "        \"\"\" Runs the Gibbs sampler on the given filename. \n",
    "        \n",
    "            The argument filename is the name and location of a .txt \n",
    "            file, which can be read in by the provided method _buildCorpus()\n",
    "            to build the corpus. Stopwords are removed if the stopwords\n",
    "            argument is provided. Note that in buildCorpus(),\n",
    "            each line of filename is considered a document.\n",
    "            \n",
    "            Initialize attributes total_nkm, total_nkv, and logprobs as\n",
    "            zero arrays.\n",
    "            total_nkm and total_nkv will be the sums of every \n",
    "            sample_rate-th nkm and nkv matrix respectively.\n",
    "            logprobs is of length burnin + sample_rate * n_samples\n",
    "            and will store each log-likelihood after each sweep of \n",
    "            the sampler.\n",
    "            \n",
    "            Burn-in the Gibbs sampler. After the burn-in, iterate further \n",
    "            for n_samples iterations, adding nkm and nkv to total_nkm and \n",
    "            total_nkv respectively, at every sample_rate-th iteration.\n",
    "            Also, compute and save the log-likelihood at each iteration \n",
    "            in logprobs using the method _loglikelihood().\n",
    "        \"\"\"\n",
    "        self._buildCorpus(filename, stopwords_file)\n",
    "        self._initialize()\n",
    "        \n",
    "        self.total_nkm = np.zeros((self.n_topics, self.n_docs))\n",
    "        self.total_nkv = np.zeros((self.n_topics, self.n_words))\n",
    "        self.logprobs = np.zeros(burnin + sample_rate * n_samples)\n",
    "        \n",
    "        for i in range(burnin):\n",
    "            # Sweep and store log likelihood\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "        for i in range(sample_rate * n_samples):\n",
    "            # Sweep and store log likelihood\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "            if not i % sample_rate:\n",
    "                # Accumulate counts\n",
    "                raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "\n",
    "                \n",
    "    def _conditional(self, m, v):\n",
    "        \"\"\" Returns the conditional distribution given m and w.\n",
    "            Called by _sweep(). \"\"\"\n",
    "        dist = (self.nkm[:, m] + self.alpha) * (self.nkv[:, v] + self.beta) / (self.nk + self.beta * self.n_words)\n",
    "        return dist / np.sum(dist)\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "        \"\"\" Computes and returns the log-likelihood. Called by sample(). \"\"\"\n",
    "        lik = 0\n",
    "\n",
    "        for k in range(self.n_topics):\n",
    "            lik += np.sum(gammaln(self.nkv[k, :] + self.beta)) - gammaln(np.sum(self.nkv[k, :] + self.beta))\n",
    "            lik -= self.n_words * gammaln(self.beta) - gammaln(self.n_words * self.beta)\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            lik += np.sum(gammaln(self.nkm[:, m] + self.alpha)) - gammaln(np.sum(self.nkm[:, m] + self.alpha))\n",
    "            lik -= self.n_topics * gammaln(self.alpha) - gammaln(self.n_topics * self.alpha)\n",
    "\n",
    "        return lik\n",
    "    \n",
    "    def _phi(self):\n",
    "        \"\"\" Initializes attribute phi. Called by topterms(). \"\"\"\n",
    "        phi = self.total_nkv + self.beta\n",
    "        self.phi = phi / np.sum(phi, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def _theta(self):\n",
    "        \"\"\" Initializes attribute theta. Called by toplines(). \"\"\"\n",
    "        theta = self.total_nkm + self.alpha\n",
    "        self.theta = theta / np.sum(theta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def topterms(self, n_terms=10):\n",
    "        \"\"\" Returns the top n_terms of each topic found. \"\"\"\n",
    "        self._phi()\n",
    "        vec = np.atleast_2d(np.arange(0, self.n_words))\n",
    "        topics = []\n",
    "        for k in range(self.n_topics):\n",
    "            probs = np.atleast_2d(self.phi[k, :])\n",
    "            mat = np.append(probs, vec, 0)\n",
    "            sind = np.array([mat[:, i] for i in np.argsort(mat[0])]).T\n",
    "            topics.append([self.vocab[int(sind[1, self.n_words - 1 - i])] \n",
    "                           for i in range(n_terms)])\n",
    "        return topics\n",
    "\n",
    "    def toplines(self, n_lines=5):\n",
    "        \"\"\" Print the top n_lines corresponding to each topic found. \"\"\"\n",
    "        self._theta()\n",
    "        lines = np.zeros((self.n_topics, n_lines))\n",
    "        for k in range(self.n_topics):\n",
    "            args = np.argsort(self.theta[:, k]).tolist()\n",
    "            args.reverse()\n",
    "            lines[k, :] = np.array(args)[0:n_lines] + 1\n",
    "        lines = lines.astype(int)\n",
    "\n",
    "        for k in range(self.n_topics):\n",
    "            print(f\"TOPIC {k + 1}\")\n",
    "            for document in lines[k]:\n",
    "                print(\" \".join(self.docs[document]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Create an `LDACGS` object with 20 topics, letting $\\alpha$ and $\\beta$ be the default values.\n",
    "Run the Gibbs sampler, with a burn-in of 100 iterations, accumulating 10 samples, only keeping the results of every 10th sweep.\n",
    "Use `stopwords.txt` as the stopwords file.\n",
    "\n",
    "Plot the log-likelihoods. How many iterations did it take to burn-in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Using the method `LDACGS.topterms()`, examine the topics for Reagan's addresses. \n",
    "\n",
    "If `n_topics=20` and `n_samples=10`, you should get the top $10$ words that represent each of the $20$ topics.\n",
    "\n",
    "Print out all $20$ topics with the associated top $10$ words. For the top $5$ topics, decide what their top $10$ words jointly represent, and come up with a label for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
