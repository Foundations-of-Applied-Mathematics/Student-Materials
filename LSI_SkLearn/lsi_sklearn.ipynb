{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Patterns in Data\n",
    "## LSI and Scikit-Learn\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from math import log\n",
    "from scipy import sparse\n",
    "from sklearn import datasets\n",
    "from scipy import linalg as la\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import linalg as spla\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "- Perform PCA on the breast cancer dataset\n",
    "- Graph the first two principal components\n",
    "- Calculate the variance captured by the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob1():\n",
    "    \"\"\"Recreate the plot in Figure 1 by performing PCA on the breast \n",
    "    cancer dataset. Translate the columns of X to have mean 0.\n",
    "    Include the amount of variance captured by the first two principal \n",
    "    components in the graph title.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Implement the function similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(i, Xhat):\n",
    "    \"\"\"\n",
    "    Takes an index and matrix representing the principal components \n",
    "    of a document and returns two indices: the index of the document \n",
    "    that is least similar and the index of the document that is most \n",
    "    similar to i.\n",
    "    \n",
    "    Parameters:\n",
    "        i: index of a document\n",
    "        Xhat: decomposed data\n",
    "    \n",
    "    Returns:\n",
    "        index_min: index of the document least similar to document i\n",
    "        index_max: index of the document most similar to document i\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_converter():\n",
    "    \"\"\"\n",
    "    Converts speech documents into an n by m array where m is the number \n",
    "    of vocabulary words and n is the number of documents.\n",
    "    \n",
    "    Returns:\n",
    "        X ((n x m) sparse matrix): Each row represents a document\n",
    "        paths (list): list where each element is a speech path \n",
    "            eg: path[0] is './Addresses/1990-Bush.txt'\n",
    "    \"\"\"\n",
    "    # Get list of filepaths to each text file in the folder.\n",
    "    folder = \"./Addresses/\"\n",
    "    paths = [folder+p for p in os.listdir(folder) if p.endswith(\".txt\")]\n",
    "\n",
    "    # Helper function to get list of words in a string.\n",
    "    def extractWords(text):\n",
    "        ignore = string.punctuation + string.digits\n",
    "        cleaned = \"\".join([t for t in text.strip() if t not in ignore])\n",
    "        return cleaned.lower().split()\n",
    "\n",
    "    # Initialize vocab set, then read each file and add to the vocab set.\n",
    "    vocab = set()\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                vocab.update(extractWords(line)) # Union sets together\n",
    "\n",
    "    # Load stopwords\n",
    "    with open(\"stopwords.txt\", 'r',  encoding=\"utf8\") as f:\n",
    "        stops = set([w.strip().lower() for w in f.readlines()])\n",
    "\n",
    "    # Remove stopwords from vocabulary, create ordering\n",
    "    vocab = {w:i for i, w in enumerate(vocab.difference(stops))}\n",
    "\n",
    "\n",
    "    counts = []      # holds the entries of X\n",
    "    doc_index = []   # holds the row index of X\n",
    "    word_index = []  # holds the column index of X\n",
    "\n",
    "    # Iterate through the documents.\n",
    "    for doc, p in enumerate(paths):\n",
    "        with open(p, 'r', encoding=\"utf8\") as f:\n",
    "            # Create the word counter.\n",
    "            ctr = Counter()\n",
    "            for line in f:\n",
    "                ctr.update(extractWords(line))\n",
    "            # Iterate through the word counter, storing counts.\n",
    "            for word, count in ctr.items():\n",
    "                if word in vocab:\n",
    "                    word_index.append(vocab[word])\n",
    "                    counts.append(count)\n",
    "                    doc_index.append(doc)\n",
    "\n",
    "    # Create sparse matrix holding these word counts.\n",
    "    X = sparse.csr_matrix((counts, [doc_index, word_index]),\n",
    "                           shape=(len(paths), len(vocab)), dtype=float)\n",
    "    return X, paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Use the function above and PCA to find speeches most and least similar to a given speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob3(speech, l=7):\n",
    "    \"\"\"\n",
    "    Uses LSI, applied to the word count matrix X, with the first 7 \n",
    "    principal components to find the most similar and least similar \n",
    "    speeches.\n",
    "\n",
    "    Parameters:\n",
    "        speech (str): Path to speech eg: \"./Addresses/1984-Reagan.txt\"\n",
    "        l (int): Number of principal components\n",
    "\n",
    "    Returns:\n",
    "        tuple of str: (Most similar speech, least similar speech)\n",
    "    \"\"\"    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this box once you have written prob3()\n",
    "speech = './Addresses/1984-Reagan.txt'\n",
    "print(prob3(speech))\n",
    "\n",
    "speech = \"./Addresses/1993-Clinton.txt\"\n",
    "print(prob3(speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Edit the end of the following function to return a weighted sparse matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_document_converter():\n",
    "    \"\"\"\n",
    "    Converts speech documents into an n by m array where m is the number \n",
    "    of vocabulary words and n is the number of documents. It gives weights\n",
    "    to the most important words in the vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        A ((n x m) sparse matrix): Each row represents a document\n",
    "        paths (list): list where each element is a speech path \n",
    "            eg: path[0] is './Addresses/1990-Bush.txt'\n",
    "    \"\"\"\n",
    "    # Get list of filepaths to each text file in the folder.\n",
    "    folder = \"./Addresses/\"\n",
    "    paths = [folder+p for p in os.listdir(folder) if p.endswith(\".txt\")]\n",
    "\n",
    "    # Helper function to get list of words in a string.\n",
    "    def extractWords(text):\n",
    "        ignore = string.punctuation + string.digits\n",
    "        cleaned = \"\".join([t for t in text.strip() if t not in ignore])\n",
    "        return cleaned.lower().split()\n",
    "\n",
    "    # Initialize vocab set, then read each file and add to the vocab set.\n",
    "    vocab = set()\n",
    "    for p in paths:\n",
    "        with open(p, 'r', encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                vocab.update(extractWords(line)) # Union sets together\n",
    "\n",
    "    # Load stopwords\n",
    "    with open(\"stopwords.txt\", 'r',  encoding=\"utf8\") as f:\n",
    "        stops = set([w.strip().lower() for w in f.readlines()])\n",
    "\n",
    "    # Remove stopwords from vocabulary, create ordering\n",
    "    vocab = {w:i for i, w in enumerate(vocab.difference(stops))}\n",
    "\n",
    "    t = np.zeros(len(vocab)) # holds global count for each word\n",
    "    counts = []      # holds the entries of X\n",
    "    doc_index = []   # holds the row index of X\n",
    "    word_index = []  # holds the column index of X\n",
    "\n",
    "    # Get doc-term counts and global term counts\n",
    "    for doc, p in enumerate(paths):\n",
    "        with open(p, 'r', encoding=\"utf8\") as f:\n",
    "            # Create the word counter.\n",
    "            ctr = Counter()\n",
    "            for line in f:\n",
    "                words = extractWords(line)\n",
    "                ctr.update(words)\n",
    "            # Iterate through the word counter, storing counts\n",
    "            for word, count in ctr.items():\n",
    "                if word in vocab:\n",
    "                    word_ind = vocab[word]\n",
    "                    word_index.append(word_ind)\n",
    "                    counts.append(count)\n",
    "                    doc_index.append(doc)\n",
    "                    t[word_ind] += count\n",
    "                    \n",
    "    # Get global weights\n",
    "    # Problem 4\n",
    "    # Student work begins here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Use the weighted_document_converter() function above and scikit-learn's PCA to find speeches most and least similar to a given speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob5(speech, l=7):\n",
    "    \"\"\"\n",
    "    Uses LSI, applied to the globally weighted word count matrix A, with \n",
    "    the first 7 principal components to find the most similar and least \n",
    "    similar speeches.\n",
    "\n",
    "    Parameters:\n",
    "        speech (str): Path to speech eg: \"./Addresses/1984-Reagan.txt\"\n",
    "        l (int): Number of principal components\n",
    "\n",
    "    Returns:\n",
    "        tuple of str: (Most similar speech, least similar speech)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this box after you have completed prob5()\n",
    "speech = './Addresses/1984-Reagan.txt'\n",
    "print(prob5(speech))\n",
    "\n",
    "speech = \"./Addresses/1993-Clinton.txt\"\n",
    "print(prob5(speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "- Split the breast cancer dataset into training and testing sets using random_state=2\n",
    "- Fit KNeighborsClassifier and RandomForestClassifier to the training data\n",
    "- Predict labels for the testing set\n",
    "- Print a classification_report for each classifier\n",
    "- Write a few sentences explaining which classifier would be better to use in this situation and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "- Use the breast cancer dataset split from Problem 6\n",
    "- Run a GridSearch with a RandomForestClassifier, modifying at least three parameters.\n",
    "- Use scoring=\"f1\"\n",
    "- Print out best_params_ and best_score_\n",
    "- Print a confusion matrix for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "- Create a pipeline with a StandardScaler, PCA, and a KNeighborsClassifier\n",
    "- Do a grid search, modifying at least 6 parameters\n",
    "- Print your best parameters and best score (f1)\n",
    "- Get a score of at least .96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
