{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Volume 3: Web Crawling</h1>\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "%matplotlib inline\n",
    "# rcParams[\"figure.figsize\"] = (16,12)    # Use this line to increase your figure size (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Write a program that accepts a web address defaulting to the http://example.webscraping.com and a list of pages defaulting to [\"/\", \"/trap\", \"/places/default/search\"].\n",
    "For each page, check if the website's robots.txt file permits access.\n",
    "Return a list of boolean values corresponding to each page, and the crawl delay time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "def prob1(url='http://example.webscraping.com', \n",
    "          pages=[\"/\", \"/trap\", \"/places/default/search\"]):\n",
    "    \"\"\"Using urllib.robotparser, check if the provided webpages are allowed\n",
    "    based on the website's robots.txt file.\n",
    "    Parameters:\n",
    "        url (str): The website's base url\n",
    "        pages (list): List of strings of webpages to check\n",
    "    Returns:\n",
    "        \"\"\"\n",
    "    raise NotImplementedError('Problem 1 Incomplete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Modify `scrape_books()` so that it gathers the price for each fiction book and returns the mean price in pounds for fiction books instead of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "def scrape_books(start_page = \"index.html\"):\n",
    "    \"\"\" Crawl through http://books.toscrape.com and extract fiction data\"\"\"\n",
    "    \n",
    "    # Initialize variables, including a regex for finding the 'next' link.\n",
    "    base_url=\"http://books.toscrape.com/catalogue/category/books/fiction_10/\"\n",
    "    titles = []\n",
    "    page = base_url + start_page                # Complete page URL.\n",
    "    next_page_finder = re.compile(r\"next\")      # We need this button.\n",
    "    \n",
    "    current = None\n",
    "\n",
    "    for _ in range(4):\n",
    "        while current == None:                   # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.  \n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find_all(class_=\"product_pod\")\n",
    "            \n",
    "        # Navigate to the correct tag and extract title.\n",
    "        for book in current:\n",
    "            titles.append(book.h3.a[\"title\"])\n",
    "    \n",
    "        # ind the URL for the page with the next data\n",
    "        if \"page-4\" not in page:\n",
    "            # Find the URL for the page with the next data.\n",
    "            new_page = soup.find(string=next_page_finder).parent[\"href\"]    \n",
    "            page = base_url + new_page      # New complete page URL.\n",
    "            current = None\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "The Internet Archive Wayback Machine archives sites so users can find the history of websites they visit and IMDB contains a variety of information on movies. Information on the top 10 box office movies of the weekend of February 21 - 23, 2020 can be found at https://web.archive.org/web/20200226124504/https://www.imdb.com/chart/boxoffice/?ref_=nv_ch_cht.\n",
    "\n",
    "Using BeaufiulSoup, Selenium, or both, return a list, with each title on a new row, of the top 10 movies of the week and order the list according to the total grossing of the movies, from most money to the least. Break ties using the weekend gross, from most money to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Project Euler (https://projecteuler.net) is a collection of mathematical computing problems.\n",
    "Each problem is listed with an ID, a description/title, and the number of users that have solved the problem.\n",
    "\n",
    "Using Selenium, BeautifulSoup, or both, record the number of people who have solved each of the 700+ problems in the archive at https://projecteuler.net/archives.\n",
    "Plot the number of people who have solved each problem against the problem IDs, using a log scale for the y-axis.\n",
    "Display the scatter plot and then print the IDs of the problems that have been solved most and least number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "The website http://watir.com/examples/simple_form.html contains an example form you can fill out.\n",
    "Using Selenium fill out the form for this example person:\n",
    "\n",
    "1. First name: Elizabeth\n",
    "2. Last name: Bennet\n",
    "3. Email: elizabeth.bennet@pemberleymail.com\n",
    "4. Interests: Dancing, Books\n",
    "5. Want to recieve newsletter: Yes\n",
    "\n",
    "Remember to use a crawl delay of at least 5 seconds so you don't send your requests too fast.\n",
    "Print out the Response text that filling out the form provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
