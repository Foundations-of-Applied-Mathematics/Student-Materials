{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Volume 3: Web Crawling</h1>\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "%matplotlib inline\n",
    "# rcParams[\"figure.figsize\"] = (16,12)    # Use this line to increase your figure size (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Write a program that accepts a web address defaulting to the http://example.webscraping.com and a list of pages defaulting to [\"/\", \"/trap\", \"/places/default/search\"].\n",
    "For each page, check if the website's robots.txt file permits access.\n",
    "Return a list of boolean values corresponding to each page, and the crawl delay time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "def prob1(url='http://example.webscraping.com', \n",
    "          pages=[\"/\", \"/trap\", \"/places/default/search\"]):\n",
    "    \"\"\"Using urllib.robotparser, check if the provided webpages are allowed\n",
    "    based on the website's robots.txt file.\n",
    "    Parameters:\n",
    "        url (str): The website's base url\n",
    "        pages (list): List of strings of webpages to check\n",
    "    Returns:\n",
    "        \"\"\"\n",
    "    raise NotImplementedError('Problem 1 Incomplete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Modify `scrape_books()` so that it gathers the price for each fiction book and returns the mean price in pounds for fiction books instead of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "def scrape_books(start_page = \"index.html\"):\n",
    "    \"\"\" Crawl through http://books.toscrape.com and extract fiction data\"\"\"\n",
    "    \n",
    "    # Initialize variables, including a regex for finding the 'next' link.\n",
    "    base_url=\"http://books.toscrape.com/catalogue/category/books/fiction_10/\"\n",
    "    titles = []\n",
    "    page = base_url + start_page                # Complete page URL.\n",
    "    next_page_finder = re.compile(r\"next\")      # We need this button.\n",
    "    \n",
    "    current = None\n",
    "\n",
    "    for _ in range(4):\n",
    "        while current == None:                   # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.  \n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find_all(class_=\"product_pod\")\n",
    "            \n",
    "        # Navigate to the correct tag and extract title.\n",
    "        for book in current:\n",
    "            titles.append(book.h3.a[\"title\"])\n",
    "    \n",
    "        # ind the URL for the page with the next data\n",
    "        if \"page-4\" not in page:\n",
    "            # Find the URL for the page with the next data.\n",
    "            new_page = soup.find(string=next_page_finder).parent[\"href\"]    \n",
    "            page = base_url + new_page      # New complete page URL.\n",
    "            current = None\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "The website IMDB contains a variety of information on movies.\n",
    "Specifically, information on the top 10 box office movies of the week can be found at https://www.imdb.com/chart/boxoffice.\n",
    "Using BeaufiulSoup, Selenium, or both, return a  list of the top 10 movies of the week and order the list according to the total grossing of the movies, from most money to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Project Euler (https://projecteuler.net) is a collection of mathematical computing problems.\n",
    "Each problem is listed with an ID, a description/title, and the number of users that have solved the problem.\n",
    "\n",
    "Using Selenium, BeautifulSoup, or both, record the number of people who have solved each of the 700+ problems in the archive at https://projecteuler.net/archives.\n",
    "Plot the number of people who have solved each problem against the problem IDs, using a log scale for the y-axis.\n",
    "Display the scatter plot and then print the IDs of the problems that have been solved most and least number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "The website http://example.webscraping.com contains a list of countries of the world.\n",
    "Using Selenium, go to the search page, enter the letters \"ca\", and hit enter or click the search button.\n",
    "Remember to use the crawl delay time you found in Problem Problem 1 so you don't send your requests too fast.\n",
    "Gather the href links associated with the < a > tags of all 10 displayed results.\n",
    "Print each link on a different line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Problem 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
